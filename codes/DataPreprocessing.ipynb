{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "respective-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "inclusive-combination",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-alcohol",
   "metadata": {},
   "source": [
    "# Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "remarkable-rabbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "class synthesisNet(nn.Module):\n",
    "    \n",
    "    def __init__(self,pathLength=40):\n",
    "        super(synthesisNet,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,1,3)\n",
    "        self.fc1 = nn.Linear((pathLength-2),1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = torch.sigmoid(self.conv1(x))\n",
    "        x = x.view(-1,self.num_flat_features(x))\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        return x\n",
    "        \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        print(x.size())\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        print(num_features)\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-saint",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "minute-spiritual",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "distant-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getColumnHeader(fileName):\n",
    "    with open(fileName, 'r') as temp_f:\n",
    "        col_count = [ len(l.split(\",\")) for l in temp_f.readlines() ]\n",
    "    column_names = [i for i in range(0, max(col_count))]\n",
    "    return column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "intermediate-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRawInputDataDF(fileName):\n",
    "    columnNames = getColumnHeader(fileName)\n",
    "    return pd.read_csv(fileName,header=None,names=columnNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-transportation",
   "metadata": {},
   "source": [
    "## Timing 1000ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "electric-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "core1_prePnR = pd.read_csv(\"dataCore/core1/prePnR.csv\")\n",
    "core1_postPnR = pd.read_csv(\"dataCore/core1/postPnR.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "embedded-tamil",
   "metadata": {},
   "outputs": [],
   "source": [
    "core1_inputData = getRawInputDataDF(\"dataCore/core1/inputFeatureFile.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-memphis",
   "metadata": {},
   "source": [
    "## Timing 800ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "aquatic-smoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "core2_prePnR = pd.read_csv(\"dataCore/core2/prePnR.csv\")\n",
    "core2_postPnR = pd.read_csv(\"dataCore/core2/postPnR.csv\")\n",
    "core2_inputData = getRawInputDataDF(\"dataCore/core2/inputFeatureFile.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-saturn",
   "metadata": {},
   "source": [
    "## Timing 600ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "formal-characteristic",
   "metadata": {},
   "outputs": [],
   "source": [
    "core3_prePnR = pd.read_csv(\"dataCore/core3/prePnR.csv\")\n",
    "core3_postPnR = pd.read_csv(\"dataCore/core3/postPnR.csv\")\n",
    "core3_inputData = getRawInputDataDF(\"dataCore/core3/inputFeatureFile.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-twenty",
   "metadata": {},
   "source": [
    "## Timing 400ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "desperate-footage",
   "metadata": {},
   "outputs": [],
   "source": [
    "core4_prePnR = pd.read_csv(\"dataCore/core4/prePnR.csv\")\n",
    "core4_postPnR = pd.read_csv(\"dataCore/core4/postPnR.csv\")\n",
    "core4_inputData = getRawInputDataDF(\"dataCore/core4/inputFeatureFile.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-haven",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-borough",
   "metadata": {},
   "source": [
    "## Common elements pre and post pnr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "general-surface",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "economic-phenomenon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCommonDF(prePnRDF,postPnRDF):\n",
    "    prePnRDF = prePnRDF.rename(columns={'pathLength': 'prePnRLen', 'pathIdx': 'pidPrePnR'})\n",
    "    postPnRDF = postPnRDF.rename(columns={'pathLength': 'postPnRLen', 'pathIdx': 'pidPostPnR'})\n",
    "    prePnRDF['pkey'] = prePnRDF['startPoint']+\",\"+prePnRDF['endPoint']\n",
    "    postPnRDF['pkey'] = postPnRDF['startPoint']+\",\"+postPnRDF['endPoint']\n",
    "    commonKeys = set(prePnRDF['pkey']) & set(postPnRDF['pkey'])\n",
    "    prePnRDF_common = prePnRDF[prePnRDF['pkey'].isin(commonKeys)]\n",
    "    postPnRDF_common = postPnRDF[postPnRDF['pkey'].isin(commonKeys)]\n",
    "    final = prePnRDF_common.merge(postPnRDF_common, on=\"pkey\", how = 'inner')\n",
    "    final[\"techmapPD\"] = final[\"techmapPD\"]*1000\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "interim-radio",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInputDataAndLabelDF(coreLabelDF,coreInputDF):\n",
    "    filteredLabelDF = coreLabelDF[['pidPrePnR','techmapPD']]\n",
    "    filteredInputDF = coreInputDF.rename(columns={0:'pidPrePnR'})\n",
    "    augmentedFilteredDF = filteredLabelDF.merge(filteredInputDF, on=\"pidPrePnR\", how = 'inner')\n",
    "    #Labelling top 10% as critical Path\n",
    "    augmentedFilteredDF.sort_values(by=[\"techmapPD\"],ascending=False)\n",
    "    augmentedFilteredDF.insert(0,'label',0)\n",
    "    for i in range(math.floor(0.1*augmentedFilteredDF.shape[0])):\n",
    "        augmentedFilteredDF.loc[i,'label']= 1\n",
    "    #Drop unncessary columns\n",
    "    augmentedFilteredDF = augmentedFilteredDF.drop(columns=['pidPrePnR','techmapPD'])\n",
    "    return augmentedFilteredDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-trade",
   "metadata": {},
   "source": [
    "## Core 1 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "antique-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "core1_df = getCommonDF(core1_prePnR,core1_postPnR)\n",
    "core1_dataset = getInputDataAndLabelDF(core1_df,core1_inputData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-geneva",
   "metadata": {},
   "source": [
    "## Core 2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "sharing-mortality",
   "metadata": {},
   "outputs": [],
   "source": [
    "core2_df = getCommonDF(core2_prePnR,core2_postPnR)\n",
    "core2_dataset = getInputDataAndLabelDF(core2_df,core2_inputData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-taylor",
   "metadata": {},
   "source": [
    "## Core 3 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "aware-column",
   "metadata": {},
   "outputs": [],
   "source": [
    "core3_df = getCommonDF(core3_prePnR,core3_postPnR)\n",
    "core3_dataset = getInputDataAndLabelDF(core3_df,core3_inputData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-marking",
   "metadata": {},
   "source": [
    "## Core 4 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "angry-korea",
   "metadata": {},
   "outputs": [],
   "source": [
    "core4_df = getCommonDF(core4_prePnR,core4_postPnR)\n",
    "core4_dataset = getInputDataAndLabelDF(core4_df,core4_inputData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "looking-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidatedData = pd.concat([core1_dataset,core2_dataset,core3_dataset,core4_dataset],axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-exhibit",
   "metadata": {},
   "source": [
    "# Dataset pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "danish-newfoundland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the columns having all the entries as NaN, and drop such columns\n",
    "count = 0\n",
    "dropColumnList = []\n",
    "for col in consolidatedData.columns:\n",
    "    if(consolidatedData[col].isnull().sum() == len(consolidatedData)):\n",
    "        count+=1\n",
    "        dropColumnList.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "systematic-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidatedData = consolidatedData.drop(columns=dropColumnList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "defined-buyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataLabel = consolidatedData['label']\n",
    "dataFeatures = consolidatedData.drop(columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "caroline-spell",
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfSamples = dataLabel.shape[0]\n",
    "maxLenOfPath = int(len(dataFeatures.columns)/3) # Three feature per gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "known-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDataFatures = dataFeatures.to_numpy().reshape(numOfSamples,1,maxLenOfPath,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "joint-champagne",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ProcessedData=np.nan_to_num(inputDataFatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "constant-prisoner",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_labels = dataLabel.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-kruger",
   "metadata": {},
   "source": [
    "# Dataset normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-south",
   "metadata": {},
   "source": [
    "## MinMax scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "large-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "minLoad = np.amin(np.amin(np.array(input_ProcessedData[:,:,:,2])[input_ProcessedData[:,:,:,2] != np.amin(input_ProcessedData[:,:,:,2])]))\n",
    "maxLoad = np.amax(input_ProcessedData[:,:,:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "another-grave",
   "metadata": {},
   "outputs": [],
   "source": [
    "minTargetCycleTime = np.amin(np.amin(np.array(input_ProcessedData[:,:,:,0])[input_ProcessedData[:,:,:,0] != np.amin(input_ProcessedData[:,:,:,0])]))\n",
    "maxTargetCycleTime = np.amax(input_ProcessedData[:,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "nonprofit-retreat",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ProcessedData[:,:,:,0] = (input_ProcessedData[:,:,:,0] - minTargetCycleTime)/(maxTargetCycleTime-minTargetCycleTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "prompt-connecticut",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ProcessedData[:,:,:,2] = (input_ProcessedData[:,:,:,2] - minLoad)/(maxLoad-minLoad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "split-satisfaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ProcessedData = input_ProcessedData.clip(min=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-consultancy",
   "metadata": {},
   "source": [
    "## Export data as numpy objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "absolute-executive",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('inputFeatures.npy',input_ProcessedData)\n",
    "np.save('outputLabels.npy',output_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
